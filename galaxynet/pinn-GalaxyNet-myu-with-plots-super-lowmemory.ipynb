{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5663c8db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5663c8db",
    "outputId": "7d8ac497-e64c-4651-d0a8-940525f10cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326e00db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "326e00db",
    "outputId": "32ce64ff-b91e-4127-fd79-46183bd95faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import imageio\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import Tensor\n",
    "from itertools import compress, cycle\n",
    "from collections import OrderedDict\n",
    "from scipy.interpolate import griddata\n",
    "from IPython.display import Image\n",
    "\n",
    "from utilities.utils import *\n",
    "\n",
    "from src.plotting import Plotter\n",
    "from src.gl_solver import GLSolver\n",
    "from src.parameters_init import ParametersInit\n",
    "from src.random_input_field import RandomInputField\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EPOCHS = 20_000\n",
    "LR = 1\n",
    "# SEED = 1234\n",
    "\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86852a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 4578.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Myus count\t 832\n",
      "Max value of myu:\t 103.75044073657139\n",
      "Min value of myu:\t 27.74673742634729\n",
      "Unique values: [27.74673742634729, 29.24175593726248, 30.974813045604947, 32.24026421684058, 32.27816441103206, 32.742499103394444, 34.12984683313881, 34.204125061456295, 34.440733654328454, 34.98022252385232, 35.74477604138318, 35.85411369681554, 35.863761736571824, 35.93376253243798, 35.98238275077933, 36.503809342874504, 36.551311091801374, 36.80982665782019, 36.98366791264197, 37.289680423175724, 37.97271374653278, 38.073514552800894, 38.208960728401415, 38.676251167676945, 38.68665999679073, 38.79164495520163, 38.823037726424964, 38.98020998881673, 38.99959958018817, 39.07661328171476, 39.12983373885473, 39.34225846176887, 39.55427086260931, 39.644762848130696, 39.73122565990835, 39.889718510251406, 39.94213992831326, 40.04137770421122, 40.193013021004134, 40.31779502970837, 40.56299276737357, 40.6901630271422, 40.70915254190766, 40.735375056261525, 40.818881823720744, 40.959983421216094, 40.99342315438779, 41.01029503954347, 41.035038440725906, 41.123526694534505, 41.123693937096014, 41.25593996909377, 41.299912850670765, 41.63329564200481, 41.655194169505066, 41.65603646060803, 41.952436367381885, 41.97691827901823, 41.98892381525075, 42.13611748942663, 42.430215477066874, 42.4990121173468, 42.69207045337672, 42.83568742521099, 43.0459186506266, 43.092668303986216, 43.17103208665441, 43.220557653632014, 43.28038222132652, 43.310818872785326, 43.33887254938376, 43.34170311111674, 43.41447228276683, 43.570342472852275, 43.58069162472636, 43.59271764813945, 43.70308503515624, 43.75048695320162, 43.763149833182666, 43.81589490144693, 43.84996984734164, 44.06383969608261, 44.31147845114322, 44.45296227644783, 44.640895465718756, 44.694866704342935, 44.76845274998527, 44.77652560152032, 44.80415585813393, 44.8254126341154, 44.827090361281606, 44.863786969692136, 45.00769231385315, 45.125564595389385, 45.2768759718327, 45.28208291464766, 45.35900099133365, 45.430538686811516, 45.51957395337684, 45.610503854722836, 45.62546745439156, 45.625726188559824, 45.706209678954124, 45.75724216217738, 45.783483853696005, 45.874400649077124, 45.88670324141061, 46.03138181695551, 46.410188663128345, 46.46277005575515, 46.52401810226533, 46.53652005600102, 46.815169371854225, 46.854475264294685, 46.86424108658881, 46.873227847928575, 46.90087714525306, 46.99359726262699, 47.107037065117495, 47.20972180742062, 47.20984441973954, 47.234979367746305, 47.26920214470819, 47.422896711808676, 47.429593071105074, 47.44115347181794, 47.49021202732475, 47.49763845250128, 47.517048734825686, 47.6054637442826, 47.605669858514474, 47.60924518952884, 47.68056678519379, 47.6875198340908, 48.01017236304259, 48.02172107154587, 48.07905134959132, 48.08545338726697, 48.15967486253707, 48.161546662177514, 48.19041421386231, 48.19736086811469, 48.2060770642398, 48.296917027210725, 48.31620043618682, 48.43954476842098, 48.469855403310014, 48.539345702404646, 48.57613152746538, 48.66256688791935, 48.707457084513955, 48.725013291822385, 48.77423543502972, 48.8235113565546, 48.82677893446342, 48.89867896498757, 48.92515350941865, 48.93243020104605, 48.935028455060454, 49.00088522803865, 49.052194158276315, 49.07290366264161, 49.11180176295113, 49.13041051002031, 49.17289615353753, 49.1901691576058, 49.203656972117855, 49.21713064123569, 49.32789916093876, 49.36169522583329, 49.38221956309393, 49.38980739411539, 49.516066809182085, 49.552387637216164, 49.58366386654268, 49.58748767864232, 49.63877297963639, 49.69093704347693, 49.70735244623816, 49.822974317516184, 49.82600131337632, 49.84612349588844, 49.86423577650001, 49.89339757642484, 49.914816588484975, 50.07022014094779, 50.07934557940599, 50.08374417614222, 50.1056486697755, 50.19333054214151, 50.25569353199666, 50.38055535888418, 50.45449992442388, 50.5797877523002, 50.58889942097413, 50.690999993869596, 50.72638003526612, 50.76925069350157, 50.87980437679799, 50.9056495519054, 50.938100379876154, 50.95875420508773, 51.026256362485064, 51.092078010571356, 51.10111136972213, 51.118847049512844, 51.14273119505649, 51.238305857643894, 51.24474654134712, 51.36174207145861, 51.362142862820825, 51.37758404502333, 51.44981002936682, 51.470619300334405, 51.47978736784972, 51.49809629741538, 51.51703080602224, 51.60122938265688, 51.694997684153506, 51.7404445061125, 51.74241264798742, 51.799898824841335, 51.83523884400364, 51.85588163882957, 51.87070212657855, 51.885184543469094, 51.91261247031469, 51.93235704646872, 51.992207154065596, 52.067609726969145, 52.075101431887205, 52.07601140016164, 52.10098324023894, 52.134054691182335, 52.19355953382916, 52.23333814474202, 52.23584661906997, 52.266644028070324, 52.282442968191894, 52.35195851698832, 52.44022977930772, 52.72295094569846, 52.723055966550284, 52.7515215787931, 52.76010167862823, 52.764190261148094, 52.84253591284453, 52.93702083628343, 52.971607999759954, 53.02528386515103, 53.031779982960586, 53.05644196488059, 53.09712166600127, 53.105965701889346, 53.1439894773294, 53.16200843271533, 53.255746674331, 53.29117047148643, 53.34132955602606, 53.41301597157367, 53.4497623381971, 53.456897569040066, 53.4606684609829, 53.526298936565794, 53.59404957136776, 53.63392946954538, 53.64288722720827, 53.65223020352151, 53.68599798905583, 53.865175599207824, 53.909631806667164, 53.91787895097195, 53.93313512063466, 54.0176863977037, 54.093145153338334, 54.12607354298335, 54.16545300976926, 54.16715308284227, 54.246079739230744, 54.25284403641034, 54.26607416366569, 54.370844775892564, 54.38009550057171, 54.435075283485205, 54.46248187874021, 54.48020908721653, 54.5067398629194, 54.577892877776044, 54.59392895511679, 54.597841014264915, 54.676870231945244, 54.7095639645251, 54.71010236929512, 54.79863226653922, 54.79871170492647, 54.84186496299707, 54.84625739014333, 54.87234011912064, 54.91002338415513, 54.913999328884884, 55.00334726541073, 55.00528626775559, 55.07090739608616, 55.13980156601163, 55.14940170915027, 55.173675605777625, 55.2873120467075, 55.31162736831205, 55.33727788282102, 55.481756478163845, 55.488782460896424, 55.52114931634479, 55.60353820347932, 55.605444481986915, 55.63749563344451, 55.70698675027843, 55.73067367307156, 55.74369896975717, 55.758956121477944, 55.883944890531794, 55.89120093637683, 55.89642981413751, 55.911094929046264, 55.92141034389318, 55.925938412173714, 55.955539030663346, 55.976085979253966, 55.98983792391925, 56.039009650540855, 56.07359556573335, 56.15647124499887, 56.184361614042366, 56.21572000390453, 56.222341570993954, 56.23510462015125, 56.28053892193261, 56.32994682898225, 56.40715011553327, 56.48708884163354, 56.49833521825215, 56.6356135772885, 56.68685349386085, 56.69787644682815, 56.749857836071484, 56.800023092349214, 56.80859604037694, 56.84234693611592, 56.84505997096671, 56.84782260530717, 56.85239139455812, 56.857221228898965, 56.85735895085365, 56.877280950515036, 56.914630405048655, 56.93005418647438, 56.955013978299824, 56.9959239166144, 57.011159153486936, 57.064109127024615, 57.1326078049271, 57.151539528993176, 57.19578590892319, 57.19665043652249, 57.23869416090153, 57.280592701464975, 57.288496338098106, 57.382849557614925, 57.391065937372645, 57.43268552531013, 57.49157096938225, 57.53588784431088, 57.55502707102873, 57.55594414463243, 57.636938338366576, 57.67208129281575, 57.71195767797053, 57.73493910206545, 57.76293074474719, 57.76298897238142, 57.77026923578169, 57.800973169695794, 57.8012596325018, 57.86873352316505, 57.90396772884303, 57.998092967147244, 58.01533787054786, 58.07918493584498, 58.151995250545454, 58.19874761852284, 58.2276568765755, 58.269822608322634, 58.27032343292638, 58.35695572452216, 58.41244178665499, 58.44647494543573, 58.51176536720296, 58.516762369013726, 58.51718225535182, 58.51725676278353, 58.52553392398443, 58.611316144945384, 58.61562445559373, 58.63570058685367, 58.65267417744244, 58.793962423116234, 58.80902180877805, 58.94655314493812, 58.9797420496834, 59.00846008822946, 59.039497195587835, 59.12149095500923, 59.13071049901041, 59.1517135493254, 59.1595211278703, 59.16124882662009, 59.185985175737315, 59.37575056470844, 59.466970499715195, 59.499327929026556, 59.50452539129957, 59.538632028448625, 59.651375365121226, 59.683162368221645, 59.76050582445788, 59.79540978012866, 59.80553512128082, 59.859647893595096, 59.860957123688316, 59.882193270028274, 59.931873311703285, 59.94519538091791, 60.027768266081296, 60.029897032033205, 60.10813557404887, 60.11377384043637, 60.1778439955365, 60.20569581108598, 60.25882943299429, 60.28030899413871, 60.28225285672014, 60.29376636616451, 60.34006410484235, 60.34755954922769, 60.36296283223923, 60.476551425797915, 60.52953899766406, 60.59526206367133, 60.63375748782588, 60.636988828581195, 60.75935741041607, 60.77000803274499, 60.79249799911414, 60.91686515664331, 60.92859880811525, 60.93610438885259, 61.03205779608223, 61.107614899378504, 61.12687338993646, 61.18808794639209, 61.19159602836946, 61.222866947185736, 61.28824896572887, 61.33879695271943, 61.43257535148908, 61.44598457076603, 61.45181347928651, 61.60865049229421, 61.67393865235931, 61.68049864123508, 61.68062186456138, 61.69338638713293, 61.71945838956262, 61.761490330925206, 61.79052957167358, 61.81339811773377, 61.87108939534795, 61.928199678740704, 61.95802778759542, 62.071508310809214, 62.08214884993321, 62.10118192687459, 62.18179294331605, 62.20381249106011, 62.23905905709445, 62.240542652258014, 62.31846429644146, 62.35970675555321, 62.39902896295832, 62.41744872775736, 62.4450433622031, 62.452748594712396, 62.480916465614904, 62.51701570888798, 62.57790901967789, 62.70593527004956, 62.736001310516954, 62.738482052241984, 62.806108239511616, 62.83215384754164, 62.83439508641791, 62.87857901222725, 62.887427811515316, 62.91469379401436, 62.971947708866445, 63.06458583452621, 63.12251361399797, 63.1266353799764, 63.15719388682973, 63.15764973643969, 63.17205685236326, 63.22318902388576, 63.26310908023666, 63.26573813788824, 63.29668069855776, 63.30749642346499, 63.353239807223076, 63.459450281841505, 63.47844682468854, 63.495595472057374, 63.55530020736363, 63.58765526076514, 63.6082066435244, 63.62399944203458, 63.67043641937846, 63.7309146608118, 63.73597987155555, 63.81300475130364, 63.82994130407255, 63.89452733158182, 63.93467324013376, 63.95710274754728, 64.14783835993116, 64.15308464008265, 64.15794452296426, 64.19963454239296, 64.34695016073522, 64.39611813648277, 64.42209205628498, 64.42946278275669, 64.48117614347997, 64.5356541662312, 64.53917793532159, 64.54587111330078, 64.63498078524232, 64.66790502762413, 64.70317985336987, 64.71377463008153, 64.74071096507396, 64.74360662922187, 64.84074087386854, 64.84482070500687, 64.87686354293302, 64.99452547696127, 65.02030698034824, 65.06479212434856, 65.07771841828117, 65.23010668483168, 65.24973427343431, 65.40204490281832, 65.41050894779451, 65.41494491900244, 65.42085920075, 65.53637340885203, 65.63662199812997, 65.68107770893987, 65.83498846864167, 65.848366780714, 65.85557472324838, 65.85581897434092, 65.86630194582712, 65.95586090141896, 65.96360870661853, 65.99472970798575, 66.07827809253939, 66.2587529827659, 66.28930275432525, 66.33784724822435, 66.34366625306346, 66.37631005658595, 66.43795487775368, 66.50799513526849, 66.51342279849472, 66.53902938639668, 66.55216519012103, 66.62184474240897, 66.6276877864383, 66.86821202554681, 66.89918243723565, 66.93257290578043, 66.94762883518888, 66.95825704947873, 66.96251745689837, 66.99310308685548, 67.02947947651627, 67.13980744159677, 67.16028045424729, 67.16951556124242, 67.17401779950079, 67.21249719377535, 67.28923082790557, 67.36760827764597, 67.459933819828, 67.5475630438873, 67.56947207374108, 67.59124447647591, 67.60579073780478, 67.61822383651543, 67.62971229015483, 67.7641365281448, 67.807453466475, 67.82169873930461, 67.85416783230511, 67.85605412152351, 67.86166039785275, 67.90814818591357, 67.91226357312355, 67.92668728394433, 67.98769796073371, 68.0278426797224, 68.10315796381525, 68.1102993877919, 68.15021696703286, 68.24347917992118, 68.26212915718129, 68.26338465739019, 68.30107848463949, 68.3301053883059, 68.44727566035175, 68.5210947864241, 68.56517590712988, 68.77434271043039, 68.77956995804219, 68.83478516776293, 68.84573001772112, 68.85233904256124, 68.96411884680603, 69.03176163331722, 69.05589331345357, 69.05952068725075, 69.10398391958763, 69.18697408563054, 69.22478276406409, 69.32725192487362, 69.36277098518325, 69.37565959451922, 69.39923817000125, 69.55601481626657, 69.60133641866624, 69.61906680662946, 69.6538173382228, 69.69538367220919, 69.70194122683735, 69.71262228116541, 69.80782636606749, 69.83992266289582, 69.8403693105412, 69.92870208922074, 69.94281335627007, 70.00531298610618, 70.01195797940724, 70.14473193353628, 70.28928170959713, 70.39433353539978, 70.40493968655568, 70.44397816360299, 70.50473184120467, 70.65037033900971, 70.7383735670034, 70.74747940780195, 70.80474880979021, 70.81261511622263, 70.9185271099044, 70.93697890715165, 70.95665290999271, 70.98971770723624, 71.11055902097537, 71.11941889072364, 71.18505941281083, 71.33401151877433, 71.40344428469484, 71.50978638013866, 71.52190498147378, 71.540255059745, 71.55930144876356, 71.58925566272148, 71.67137292789215, 71.77334857822134, 71.79934932656765, 71.80504594083338, 71.94630653402903, 72.0244786184189, 72.04236671341414, 72.09829396203277, 72.25778077338634, 72.36459719265038, 72.42424436252368, 72.53340097372428, 72.54617317943065, 72.55640963174683, 72.56541938785846, 72.66725816521758, 72.67562109276231, 72.71028752833614, 72.78628377036414, 72.8781314867408, 72.92003726086377, 73.00669975726278, 73.03956870529552, 73.06359314394022, 73.07745332094656, 73.43752587160587, 73.65700766504114, 73.70472259302646, 73.72488825156549, 73.74602065597749, 73.77800375465569, 73.91571734766295, 74.01719569107408, 74.03679140019891, 74.13220768509456, 74.28010509041587, 74.31679793098813, 74.54709646051471, 74.55304143802965, 74.57870483887042, 74.57983608618206, 74.67028254070107, 74.68951155644903, 74.72528283188022, 74.77847918739259, 74.79185063259068, 74.89409419797698, 74.92394347630766, 74.98454631323919, 75.17812413949483, 75.22387425951537, 75.32368397827649, 75.34879336693193, 75.4761976800375, 75.63985702331267, 75.71312157514379, 75.78038972792184, 75.84942771433879, 76.14122331780845, 76.21542569880998, 76.33407733624172, 76.49838243388933, 76.52628097426381, 76.65898251151806, 76.7807310024469, 76.90421091388404, 76.96245003861782, 76.97113901894815, 76.97775584930115, 77.10612499600525, 77.340917833306, 77.51264075950527, 77.57325864672363, 77.62007712365242, 77.6418819825856, 77.7810291267632, 77.85623956259433, 77.86437912870211, 78.24894925690955, 78.65467443519428, 78.71693629099916, 78.74685196007843, 78.78517722649562, 78.882943802385, 78.96805133033119, 79.09523195093254, 79.30160067848438, 79.42570489008337, 79.64424524969245, 79.77187418341268, 79.81912684043793, 79.86507476678068, 80.0995328671102, 80.10910571529892, 80.40702463837066, 80.86444617872904, 80.87330957538262, 81.10273328465124, 81.44563800860844, 81.53104671377118, 81.53830105487486, 81.60682006799253, 81.74356475767678, 82.17260181177481, 82.19762807103008, 82.3969153710847, 82.48055685128107, 82.49792766199535, 82.55358940930712, 82.61290462499034, 82.92157948860999, 83.50773430196931, 83.52726861954454, 83.61772221624096, 83.81803386271366, 84.00090362900498, 84.27460233822593, 84.41150020251703, 84.48688451852928, 84.61351731448104, 84.97005481347833, 85.03064856644771, 85.39260835642956, 85.794237580964, 86.0131913524558, 86.23215559153473, 86.28992252410463, 86.31056487753509, 86.51908642223866, 86.65329544891526, 87.34683102684725, 87.46669872872204, 87.94889016886138, 88.06654656936875, 88.45845139647525, 89.06579119740802, 90.1644103971191, 90.19062121810703, 90.47916339717968, 91.68763538151971, 91.97760682426014, 93.1676656742706, 95.00945554790084, 95.67426908176384, 95.93261930656577, 99.6743660268445, 103.75044073657139]\n",
      "Counts:\t\t [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128  64\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128  64\n",
      " 128 128  64 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128  64 128 128 128 128 128 128 128 128 128 128 128  64\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128  64 128 128  64 128  64 128 128 128\n",
      " 128 128 128  64 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128  64 128 128 128 128 128 128 128  64 128 128 128 128 128\n",
      " 128 128 128 128 128  64  64 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128  64 128 128  64 128  64  64 128 128 128 128  64 128 128 128 128 128\n",
      " 128 128  64 128 128 128 128 128  64 128 128 128 128  64 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128  64\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128  64 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128  64\n",
      " 128 128  64 128 128 128 128 128 128 128 128 128 128 128 128  64 128 128\n",
      " 128 128 128 128  64 128 128 128 128 128 128  64 128 128 128  64 128 128\n",
      " 128 128 128  64 128 128 128 128 128 128 128 128 128 128  64 128 128 128\n",
      "  64  64 128 128 128 128 128 128  64 128 128 128 128  64 128 128 128 128\n",
      " 128 128 128 128 128  64 128 128 128 128  64 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128  64 128 128 128 128 128  64 128 128 128\n",
      " 128 128  64 128  64 128 128 128  64 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128  64 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128  64 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      "  64 128 128  64 128  64 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128  64 128 128 128\n",
      " 128 128 128 128 128 128 128  64  64 128 128  64 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128  64 128 128 128 128 128 128 128 128 128 128  64\n",
      " 128 128 128 128 128 128 128 128 128 128 128  64 128 128  64 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      "  64 128 128 128 128 128  64 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n",
      " 128 128 128 128 128 128 128 128 128 128 128  64 128 128 128  64 128 128\n",
      "  64 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128  64\n",
      " 128 128 128 128 128  64 128 128 128 128 128 128  64 128 128 128 128 128\n",
      " 128 128  64 128]\n",
      "A.shape=(1, 100, 32, 32),\n",
      "Myu.shape=(1, 100, 32, 32),\n",
      "\n",
      "Any NaN values in Myu\t\t False\n",
      "Any NaN values in A_original\t False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"galaxynet_myu_with_plots_superL20\"\n",
    "mtlibpath_prefix =\"galaxynet_myu_with_plots_super_mtlL20\"\n",
    "\n",
    "Nx=256\n",
    "Ny=256\n",
    "\n",
    "Lx= 20\n",
    "Ly= 20\n",
    "\n",
    "T_end = 1\n",
    "dt = 0.01\n",
    "\n",
    "N_ITERATIONS = int(T_end / dt)\n",
    "A_norm, A_original, mem_rate, myu_original = compute_A_norm(\n",
    "    Nx=Nx, \n",
    "    Ny=Ny, \n",
    "    input_to_defect_ratio = 4*4, \n",
    "    mean=5.4, \n",
    "    std_deviation=0.8, \n",
    "    time_period=80, \n",
    "    Lx=Lx, \n",
    "    Ly=Ly, \n",
    "    dt=dt, \n",
    "    T_End=T_end, \n",
    "    parallel_runs=1, \n",
    "    input_scale=0.75, \n",
    "    mem_coef=1, \n",
    "    time_period_parameter=8, \n",
    "    _mean=5.4, \n",
    "    std_deviation_run_computation=0.8,\n",
    "    input_myu=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1D-Md_qQdSn-",
   "metadata": {
    "id": "1D-Md_qQdSn-"
   },
   "source": [
    "GETTING HANDS DIRTY WITH THE NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yp-gI3_HdVWJ",
   "metadata": {
    "id": "yp-gI3_HdVWJ"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, Lx, Nx).flatten()[:, None]\n",
    "y = np.linspace(0, Ly, Ny).flatten()[:, None]\n",
    "t = np.linspace(0, T_end, N_ITERATIONS).flatten()[:, None]\n",
    "\n",
    "Exact = A_original.squeeze(0)\n",
    "\n",
    "X, T, Y = np.meshgrid(x, t, y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()\n",
    "u_star = np.hstack([u_star.real[:, None],u_star.imag[:, None]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9462a89",
   "metadata": {
    "id": "c9462a89"
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial_{t} A &= \\mu A+\\Delta A-|A|^{2} A\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bad1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GALAXYNET(nn.Module):\n",
    "    def __init__(self, layers_list, activation_function_list = None, linm = None):\n",
    "        super(GALAXYNET, self).__init__()\n",
    "        self._depth = len(layers_list) - 1\n",
    "        \n",
    "        if activation_function_list is None:\n",
    "            activation_function_list = [F.softplus for _ in range(self._depth - 1)]\n",
    "            \n",
    "        if linm is None:\n",
    "            linm =  np.tril(np.ones(self._depth + 1, dtype = int))\n",
    "        lin = linm@layers_list\n",
    "        \n",
    "        self._activation_function_list = activation_function_list\n",
    "        \n",
    "        self._Wtmx = nn.Sequential(*[torch.nn.Linear(lin[i], layers_list[i+1], dtype = torch.float64) for i in range(self._depth)])\n",
    "        self._linm = linm\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam( params = self._Wtmx.parameters(), lr=0.01 )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        layers = [x,self._Wtmx[0](x)]\n",
    "        for i in range(1, self._depth):\n",
    "            layers[i] = self._activation_function_list[i-1](layers[i])\n",
    "            ind = self._linm[i]\n",
    "            inpind = np.where(ind)[0]\n",
    "            inp = torch.concat([layers[i] for i in inpind], dim = -1)\n",
    "            layers.append(self._Wtmx[i](inp))\n",
    "        return layers[-1]        \n",
    "\n",
    "    def predict(self, x):\n",
    "        self._Wtmx.eval()\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.tensor(x, dtype = torch.float64).to(device)\n",
    "        y =  self.forward(x).cpu().detach().numpy()\n",
    "        return y[:,0] + y[:,1]*1j    \n",
    "\n",
    "    def rmsef(self, y, y_pred):\n",
    "        mseloss = torch.sum((y_pred - y)**2, dim = 1)\n",
    "        return torch.mean(torch.sqrt(mseloss))   \n",
    "\n",
    "    def msef(self, y, y_pred):\n",
    "        return torch.mean((y_pred - y)**2)       \n",
    "\n",
    "    def fastmsebatchtrain(self, x, y, epochs=100, batch_size = 64):\n",
    "        \n",
    "        x = torch.tensor(x, dtype = torch.float64).to(device)\n",
    "        y = torch.tensor(y, dtype = torch.float64).to(device)\n",
    "        dataloader = DataLoader(dataset = torch.hstack((x,y)), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        L = []\n",
    "        \n",
    "        pbar = tqdm(total=epochs)\n",
    "        try:\n",
    "            while True:\n",
    "                if pbar.n >= epochs:\n",
    "                    break\n",
    "                for tmp in dataloader:\n",
    "                    (tmpx, tmpy, tmpt, tmpu_real, tmpu_img) = tmp.T\n",
    "                    X = torch.stack((tmpx,tmpy,tmpt)).T\n",
    "                    U = torch.stack((tmpu_real, tmpu_img)).T\n",
    "\n",
    "                    y_pred = self.forward(X)\n",
    "                    loss = self.msef(y_pred,U)\n",
    "                    L.append(loss.cpu().detach().numpy())\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    self._Wtmx.zero_grad()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    # Update the progress bar\n",
    "                    if pbar.n >= epochs:\n",
    "                        break\n",
    "                    pbar.update(1)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            raise Exception(e)\n",
    "        finally:\n",
    "            # Close the progress bar\n",
    "            pbar.close()          \n",
    "        return L\n",
    "    \n",
    "    def fastrmsebatchtrain(self, x, y, epochs=100, batch_size = 64):\n",
    "        \n",
    "        x = torch.tensor(x, dtype = torch.float64).to(device)\n",
    "        y = torch.tensor(y, dtype = torch.float64).to(device)\n",
    "        dataloader = DataLoader(dataset = torch.hstack((x,y)), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        L = []\n",
    "        \n",
    "        pbar = tqdm(total=epochs)\n",
    "        try:\n",
    "            while True:\n",
    "                if pbar.n >= epochs:\n",
    "                    break\n",
    "                for tmp in dataloader:\n",
    "                    (tmpx, tmpy, tmpt, tmpu_real, tmpu_img) = tmp.T\n",
    "                    X = torch.stack((tmpx,tmpy,tmpt)).T\n",
    "                    U = torch.stack((tmpu_real, tmpu_img)).T\n",
    "\n",
    "                    y_pred = self.forward(X)\n",
    "                    loss = self.rmsef(y_pred,U)\n",
    "                    L.append(loss.cpu().detach().numpy())\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    self._Wtmx.zero_grad()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    # Update the progress bar\n",
    "                    if pbar.n >= epochs:\n",
    "                        break\n",
    "                    pbar.update(1)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            raise Exception(e)\n",
    "        finally:\n",
    "            # Close the progress bar\n",
    "            pbar.close()          \n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4036c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GALAXYPINN(GALAXYNET):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super(GALAXYPINN, self).__init__(*args,**kwargs)\n",
    "#         myu = torch.randn(4, 2, dtype=torch.float64).to(device)\n",
    "#         myu = nn.Parameter(myu)\n",
    "#         self._Wtmx.register_parameter('myu', myu)\n",
    "#         self.myuparam = myu\n",
    "#         myu = transform_and_stack(myu, 4, 200).to(device).clone().requires_grad_(True)\n",
    "#         self.myu = myu.view(200, 4, 4)\n",
    "        self.myureset()\n",
    "    \n",
    "    def myureset(self):\n",
    "        myu = torch.randn(mem_rate, Nx, Ny, dtype=torch.float64).to(device)\n",
    "        myu = torch.abs(myu)\n",
    "        myu = nn.Parameter(myu)\n",
    "        self._Wtmx.register_parameter('myu', myu)\n",
    "        self.myuparam = myu\n",
    "        self.myu = myu\n",
    "    \n",
    "    def loaddata_precalculate(self,x):\n",
    "        myuloss = MYULOSS(*tuple(x.T),self)\n",
    "        myuloss.calculate_f_withoutmyu()\n",
    "        self.myuloss = myuloss\n",
    "    \n",
    "    def myutrain(self, epochs=100, lr = 0.01):\n",
    "        myuoptimizer = torch.optim.Adam( params = [self.myuparam], lr=lr ) \n",
    "        myuoptimizer.zero_grad()\n",
    "\n",
    "        for _ in tqdm(range(epochs)):\n",
    "            self.myuloss.fmse(self.myu).backward(retain_graph=True)\n",
    "            myuoptimizer.step()\n",
    "            myuoptimizer.zero_grad()\n",
    "        \n",
    "        FMSE = self.myuloss.FMSE\n",
    "        self.myuloss.clear()\n",
    "        return FMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc59492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYULOSS:\n",
    "  def __init__(self, x, y, t, net, verbose = 0):\n",
    "      self.msef = nn.MSELoss()\n",
    "      self.FMSE = []\n",
    "      self.x = x\n",
    "      self.y = y\n",
    "      self.t = t\n",
    "      self.net = net\n",
    "        \n",
    "  def plot(self, title= 'MYU training'):\n",
    "    plt.plot(self.FMSE)\n",
    "    plt.yscale('log')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('FMSE')\n",
    "    self.clear()\n",
    "    \n",
    "  def clear(self):\n",
    "    self.FMSE = []\n",
    "    \n",
    "  def fmse(self, myu):\n",
    "    f_loss = torch.mean(torch.abs(self.net_f(myu)) ** 2)\n",
    "    self.FMSE.append(f_loss.cpu().detach().numpy())\n",
    "    return f_loss\n",
    "  \n",
    "  def net_f(self, myu, verbose = 0):\n",
    "        return self.f_withoutmyu - myu*self.u \n",
    "    \n",
    "  def calculate_f_withoutmyu(self):\n",
    "        x,y,t = self.x, self.y, self.t\n",
    "        \n",
    "        u, u_t,u_xx,u_yy = MYULOSS.pref(x,y,t,net)\n",
    "        \n",
    "        self.u = u.cpu().detach()\n",
    "            \n",
    "        f_withoutmyu = u_t - (u_xx + u_yy) + torch.pow(torch.abs(u), 2)*u #- myu*u\n",
    "        self.f_withoutmyu = f_withoutmyu.cpu().detach()\n",
    "        free_memory(u_t, u_xx, u_yy, u, f_withoutmyu)\n",
    "\n",
    "    \n",
    "  def f_withoutmyu(x,y,t,ru,iu):\n",
    "        (ru_t, ru_x, ru_y) = torch.autograd.grad(ru, (t, x, y), grad_outputs=torch.ones_like(ru), create_graph=True, retain_graph=True)\n",
    "        (iu_t, iu_x, iu_y) = torch.autograd.grad(iu, (t, x, y), grad_outputs=torch.ones_like(iu), create_graph=True, retain_graph=True)\n",
    "\n",
    "        (ru_xx,) = torch.autograd.grad(ru_x, (x), grad_outputs=torch.ones_like(ru_x), create_graph=True)\n",
    "        (iu_xx,) = torch.autograd.grad(iu_x, (x), grad_outputs=torch.ones_like(iu_x), create_graph=True)\n",
    "\n",
    "        (ru_yy,) = torch.autograd.grad(ru_y, (y), grad_outputs=torch.ones_like(ru_y), create_graph=True)\n",
    "        (iu_yy,) = torch.autograd.grad(iu_y, (y), grad_outputs=torch.ones_like(iu_y), create_graph=True)\n",
    "\n",
    "        u =( ru + iu * 1j)\n",
    "        u_t = (ru_t + iu_t * 1j)\n",
    "        u_xx =( ru_xx + iu_xx *1j)\n",
    "        u_yy = (ru_yy + iu_yy *1j)\n",
    "        \n",
    "        return u, u_t,u_xx,u_yy\n",
    "    \n",
    "  def pref(x,y,t, net, batch_size = 8192):\n",
    "    dataloader = DataLoader(dataset = X_star, batch_size = batch_size, shuffle=False)\n",
    "    cache = {\n",
    "        'u':[],\n",
    "        'u_t':[],\n",
    "        'u_xx':[],\n",
    "        'u_yy':[],\n",
    "    }\n",
    "    for tmp in tqdm(dataloader):\n",
    "        x,y,t = torch.tensor(tmp.T, dtype = torch.float64, requires_grad=True).to(device)\n",
    "        ru,iu = net.forward(torch.stack((x,y,t)).T).T\n",
    "        u, u_t,u_xx,u_yy = MYULOSS.f_withoutmyu(x,y,t,ru,iu)\n",
    "        cache['u'].append(u.cpu().detach())\n",
    "        cache['u_t'].append(u_t.cpu().detach())\n",
    "        cache['u_xx'].append(u_xx.cpu().detach())\n",
    "        cache['u_yy'].append(u_yy.cpu().detach())\n",
    "        \n",
    "    return  torch.cat(cache['u']).view(mem_rate, Nx, Ny), \\\n",
    "            torch.cat(cache['u_t']).view(mem_rate, Nx, Ny),\\\n",
    "            torch.cat(cache['u_xx']).view(mem_rate, Nx, Ny),\\\n",
    "            torch.cat(cache['u_yy']).view(mem_rate, Nx, Ny)\n",
    "\n",
    "      \n",
    "def free_memory(*variables):\n",
    "    del variables\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315c3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = 'cpu'\n",
    "net = GALAXYPINN([3,8,32,64,32,8,2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca6fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|█████████████▊                     | 39438/100000 [00:39<01:00, 994.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m L1 \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfastrmsebatchtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mGALAXYNET.fastrmsebatchtrain\u001b[0;34m(self, x, y, epochs, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m L\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Wtmx\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    307\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L1 = net.fastrmsebatchtrain(x = X_star, y = u_star, epochs = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0311c5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(L1)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Custom Loss')\n",
    "plt.title('Training of the AllInputNet \\n lr=0.01')\n",
    "plt.savefig(f'{mtlibpath_prefix}_allinputnet001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.optimizer.param_groups[0]['lr'] = 0.001\n",
    "L2 = net.fastrmsebatchtrain(x = X_star, y = u_star, epochs = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f765c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(L2)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Custom Loss')\n",
    "plt.title('Training of the AllInputNet \\n lr=0.001')\n",
    "plt.savefig(f'{mtlibpath_prefix}_allinputnet0001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c2532",
   "metadata": {},
   "source": [
    "## Save model for later training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"fastrmsebatchtrained.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d783c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "net =  GALAXYPINN([3,8,32,64,32,8,2])\n",
    "net.load_state_dict(torch.load('fastrmsebatchtrained.net'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd2ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net=net.to(device)\n",
    "\n",
    "net.optimizer.param_groups[0]['lr'] = 0.0003\n",
    "L3 = net.fastrmsebatchtrain(x = X_star, y = u_star, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbeeb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Custom Loss')\n",
    "plt.title('Training of the AllInputNet \\n lr=0.0003')\n",
    "plt.savefig(f'{mtlibpath_prefix}_allinputnet00003.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea994c61",
   "metadata": {},
   "source": [
    "## calculating myus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c8864",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "net.loaddata_precalculate(X_star)\n",
    "net.myureset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aba25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "net = net.to(device)\n",
    "figure, axes = plt.subplots(nrows = 2,ncols = 2, figsize=(8, 4.5))\n",
    "\n",
    "for lr, ax in zip([10,3,1,0.3],np.array(axes).flatten()):\n",
    "    L = net.myutrain(lr = lr, epochs = 10)\n",
    "    ax.plot(L)    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'lr={lr}')\n",
    "\n",
    "figure.text(0.02, 0.5, 'FMSE', ha='center', va='center', rotation='vertical')\n",
    "figure.text(0.5, 0.002, 'epochs', ha='center', va='center')\n",
    "figure.suptitle('MYU Training', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{mtlibpath_prefix}_myutraining.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2c683",
   "metadata": {},
   "source": [
    "## Visualizing and saving plot gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploter = Plotter(net.myu.cpu().detach().numpy())\n",
    "ploter.output_animation(mem_rate, save_gif=True, file_name=rf\"{path}_myupred.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888b150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ploter = Plotter(myu_original.squeeze(0))\n",
    "ploter.output_animation(mem_rate, save_gif=True, file_name=rf\"{path}_myuorig.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf2c78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3dcf2c78",
    "outputId": "6c707713-09bf-46bf-d2b2-f0ceb7920883",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_gifs(\n",
    "    memory_rate=mem_rate,\n",
    "    u_pred=net.predict(X_star),\n",
    "    original=A_original,\n",
    "    save=True,\n",
    "    path_for_gif=path+\".gif\",\n",
    "    duration=500,\n",
    "    title=\" \"\n",
    ")\n",
    "Image(filename=path+\".gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158cc3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif = imageio.mimread(path+\".gif\", memtest=False)\n",
    "nums = len(gif)\n",
    "print(\"Total {} frames in the gif {}!\".format(nums, path+\".gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141eea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert form BGR to RGB\n",
    "imgs = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in gif]\n",
    "\n",
    "# Save frames to video\n",
    "out = cv2.VideoWriter(path+'.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 2, (imgs[0].shape[1], imgs[0].shape[0]))\n",
    "\n",
    "for img in imgs:\n",
    "    out.write(img)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14989401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb832ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc353247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d619f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92851f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf859788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
