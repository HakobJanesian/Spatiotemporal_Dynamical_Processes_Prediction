{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326e00db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import imageio\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import Tensor\n",
    "#from lion_pytorch import Lion\n",
    "from itertools import compress, cycle\n",
    "from collections import OrderedDict\n",
    "from scipy.interpolate import griddata\n",
    "from IPython.display import Image\n",
    "\n",
    "from utilities.utils import *\n",
    "from src.plotting import Plotter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3df54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx=4\n",
    "Ny=4\n",
    "\n",
    "Lx=30 \n",
    "Ly=30\n",
    "\n",
    "T_end = 1\n",
    "dt = 0.005\n",
    "\n",
    "N_ITERATIONS = int(T_end / dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 6667.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Myus count\t 8\n",
      "Max value of myu:\t 104.14104515595743\n",
      "Min value of myu:\t 28.893477672414097\n",
      "Unique values: [28.893477672414097, 53.20229459360476, 58.107982450278136, 66.83168520679922, 73.33809836951713, 81.01192118026601, 85.93150003649163, 104.14104515595743]\n",
      "Counts:\t\t [400 400 400 400 400 400 400 400]\n",
      "A.shape=(1, 200, 4, 4),\n",
      "Myu.shape=(1, 200, 4, 4),\n",
      "\n",
      "Any NaN values in Myu\t\t False\n",
      "Any NaN values in A_original\t False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "A_norm, A_original, mem_rate, myu_original = compute_A_norm(\n",
    "    Nx=Nx, \n",
    "    Ny=Ny, \n",
    "    input_to_defect_ratio=2*2, \n",
    "    mean=5.4, \n",
    "    std_deviation=0.8, \n",
    "    time_period=25, \n",
    "    Lx=Lx, \n",
    "    Ly=Ly, \n",
    "    dt=dt, \n",
    "    T_End=T_end, \n",
    "    parallel_runs=1, \n",
    "    input_scale=0.75, \n",
    "    mem_coef=1, \n",
    "    time_period_parameter=100, \n",
    "    _mean=5.4, \n",
    "    std_deviation_run_computation=1,\n",
    "    input_myu=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d8e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3b215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df15d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de51c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_plot(data, mem_rat, save_fig=False, file_name=\"A_in_norm_80%04d.png\"):\n",
    "    fig = plt.figure(1, figsize=(6.5, 6.5))\n",
    "    for index in tqdm(range(mem_rat)):\n",
    "        if index % 2 == 0:\n",
    "            plt.clf()\n",
    "            plt.imshow(data[index, :, :], origin='lower', vmax=0.9*np.max(data))\n",
    "            plt.colorbar()\n",
    "            if save_fig:\n",
    "                filename = file_name % (index/2)\n",
    "                plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf5c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 37.86it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 650x650 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_plot(A_norm, mem_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b9b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self._depth = len(layers) - 1\n",
    "        self._activation_function = torch.nn.Tanh\n",
    "        layers_list = list()\n",
    "        \n",
    "        for i in range(self._depth - 1): \n",
    "            layers_list.append(('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1], dtype=torch.complex64)))\n",
    "            layers_list.append(('activation_%d' % i, self._activation_function()))\n",
    "            \n",
    "        layers_list.append(('layer_%d' % (self._depth - 1), torch.nn.Linear(layers[-2], layers[-1], dtype=torch.complex64)))\n",
    "        layerDict = OrderedDict(layers_list)\n",
    "        self._layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self._layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811d47e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNetwork(\n",
       "  (_layers): Sequential(\n",
       "    (layer_0): Linear(in_features=3, out_features=20, bias=True)\n",
       "    (activation_0): Tanh()\n",
       "    (layer_1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_1): Tanh()\n",
       "    (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_2): Tanh()\n",
       "    (layer_3): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_3): Tanh()\n",
       "    (layer_4): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_4): Tanh()\n",
       "    (layer_5): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_5): Tanh()\n",
       "    (layer_6): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn = SimpleNeuralNetwork([3, 20, 20, 20, 20, 20, 20, 1])\n",
    "dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9462a89",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial_{t} A &= \\mu A+\\Delta A-|A|^{2} A\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02886749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, optimizer_type, params, **kwargs):\n",
    "        self.defaults = kwargs\n",
    "        super(ComplexOptimizer, self).__init__(params, self.defaults)\n",
    "\n",
    "        # Split complex parameters into real and imaginary parts\n",
    "        real_params = []\n",
    "        imag_params = []\n",
    "        self.is_imag = []\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group[\"params\"]:\n",
    "                if param.is_complex():\n",
    "                    real_param = param.real.clone().detach().requires_grad_(True)\n",
    "                    imag_param = param.imag.clone().detach().requires_grad_(True)\n",
    "                    real_params.append(real_param)\n",
    "                    imag_params.append(imag_param)\n",
    "                    self.is_imag.append(True)\n",
    "                else:\n",
    "                    real_params.append(param)\n",
    "                    self.is_imag.append(False)\n",
    "\n",
    "        # Create two optimizers, one for real parts and one for imaginary parts\n",
    "        self.real_optimizer = optimizer_type(real_params, **self.defaults)\n",
    "        self.imag_optimizer = optimizer_type(imag_params, **self.defaults)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure):\n",
    "        # Define a closure for real parts\n",
    "        def real_closure():\n",
    "            real_loss = closure()\n",
    "            if real_loss is not None:\n",
    "                real_loss.backward(retain_graph=True)\n",
    "            return real_loss\n",
    "\n",
    "        # Define a closure for imaginary parts\n",
    "        def imag_closure():\n",
    "            imag_loss = closure()\n",
    "            if imag_loss is not None:\n",
    "                imag_loss.backward()\n",
    "            return imag_loss\n",
    "\n",
    "        # Optimize real and imaginary parts separately\n",
    "        with torch.enable_grad():\n",
    "            real_loss = self.real_optimizer.step(real_closure)\n",
    "            imag_loss = self.imag_optimizer.step(imag_closure)\n",
    "\n",
    "        # Update the original complex parameters with the optimized real and imaginary parts\n",
    "        for param_group, real_param_group, imag_param_group in zip(self.param_groups, self.real_optimizer.param_groups, self.imag_optimizer.param_groups):\n",
    "            for param, real_param, imag_param in zip(compress(param_group[\"params\"], self.is_imag), compress(real_param_group[\"params\"],self.is_imag), imag_param_group[\"params\"]):\n",
    "                if param.is_complex():\n",
    "                    with torch.no_grad():\n",
    "                        param.copy_(torch.complex(real_param, imag_param))\n",
    "\n",
    "        return real_loss + imag_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908f9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory(*variables):\n",
    "    del variables\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be46b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    def __init__(self, X, u, layers, lb, ub, lambda_reg=None, optimizer=Lion, myu_bound=1, clipping=False):\n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb, dtype=torch.int64).to(device)\n",
    "        self.ub = torch.tensor(ub, dtype=torch.int64).to(device)\n",
    "        \n",
    "        # data\n",
    "        self.x = torch.tensor(X[:, 0:1], requires_grad=True, dtype=torch.complex64).reshape(200, 4, 4).to(device)\n",
    "        self.y = torch.tensor(X[:, 1:2], requires_grad=True, dtype=torch.complex64).reshape(200, 4, 4).to(device)\n",
    "        self.t = torch.tensor(X[:, 2:3], requires_grad=True, dtype=torch.complex64).reshape(200, 4, 4).to(device)          \n",
    "        self.u = torch.tensor(u, requires_grad=True, dtype=torch.complex64).reshape(200, 4, 4).to(device)\n",
    "           \n",
    "        # self.myu = torch.randn(2, 4, dtype=torch.float64, requires_grad=True)\n",
    "        # # self.myu.data = transform_and_stack(self.myu.data, 10, 25).flatten().\n",
    "        # # self.myu = torch.nn.Parameter(self.myu.to(device))\n",
    "        # self.myu = transform_and_stack(self.myu.to(device), 4, 25)\n",
    "\n",
    "        # # simple neural network  \n",
    "        # self.dnn = SimpleNeuralNetwork(layers).to(device)\n",
    "        # self.dnn.register_parameter('myu', torch.randn(2, 4, dtype=torch.float64, requires_grad=True))\n",
    "        # self.dnn.myu.data = nn.Parameter(torch.randn(2, 4).to(device))\n",
    "        \n",
    "        # Create a tensor with random values\n",
    "        self.myu = torch.randn(4, 2, dtype=torch.float64, requires_grad=True).to(device)\n",
    "        self.myu = nn.Parameter(self.myu)\n",
    "        self.dnn = SimpleNeuralNetwork(layers).to(device)\n",
    "        self.dnn.register_parameter('myu', self.myu)\n",
    "\n",
    "        # self.dnn.myu = nn.Parameter(transformed_myu_tensor).to(device)\n",
    "        \n",
    "        params_list = self.dnn.parameters()\n",
    "        \n",
    "        self.optimizer = ComplexOptimizer(\n",
    "            optimizer_type=optimizer,   # Lion, LBFGS\n",
    "            params=params_list,\n",
    "            lr=1.4,                                      # Learning Rate\n",
    "            max_iter=1200,                                # Default max # of iterations per optimization step                                                                    #\n",
    "            tolerance_grad=1e-2,                        # Default termination tolerance on first order optimality\n",
    "            tolerance_change=1e-2,                      # Default termination tolerance on function value/parameter changes\n",
    "            history_size=1200\n",
    "        )\n",
    "        \n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr=1.2)        \n",
    "        self.optimizer_counter = 0\n",
    "        \n",
    "        self.history = {\n",
    "            'u_loss': [],\n",
    "            'f_loss': [],\n",
    "            'total_loss': [],\n",
    "            'myu': []\n",
    "        }\n",
    "        \n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.myu_bound = myu_bound\n",
    "\n",
    "        self.initial_f_loss = None\n",
    "        self.initial_u_loss = None\n",
    "        \n",
    "        self.clipping = clipping\n",
    "        \n",
    "        # print(\"u\", self.u.shape)\n",
    "        # print(\"x\", self.x.shape)\n",
    "        # print(\"y\", self.y.shape)\n",
    "        # print(\"t\", self.t.shape)\n",
    "        \n",
    "    def net_u(self, x, y, t):\n",
    "        u = self.dnn(torch.cat([x, y, t], dim=1).view(-1, 3))\n",
    "        return u\n",
    "   \n",
    "    def clip(self, x, lower_bound, upper_bound):\n",
    "        return torch.clamp(x, min=lower_bound, max=upper_bound)\n",
    "    \n",
    "    def update_myu(self):\n",
    "        with torch.no_grad():\n",
    "            self.myu.data = self.clip(self.myu, 0.0, self.myu_bound)\n",
    "    \n",
    "    def net_f(self, x, y, t):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        u = self.net_u(x, y, t).view(200, 4, 4)\n",
    "     \n",
    "        myu = self.myu\n",
    "        # myu = transform_and_stack(myu, 4, 200).to(device)\n",
    "        myu = transform_and_stack(myu, 4, 200).to(device).clone().requires_grad_(True)\n",
    "        myu = myu.view(200, 4, 4)\n",
    "\n",
    "        # print(myu.shape)\n",
    "        # myu.requires_grad=True\n",
    "\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_y = torch.autograd.grad(\n",
    "            u, y, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_yy = torch.autograd.grad(\n",
    "            u_y, y,\n",
    "            grad_outputs=torch.ones_like(u_y),\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        # print(\"u\", u.shape)\n",
    "        # print(\"u_t\", u_t.shape)\n",
    "        # print(\"u_x\", u_x.shape)\n",
    "        # print(\"u_y\", u_y.shape)\n",
    "        # print(\"u_xx\", u_xx.shape)\n",
    "        # print(\"u_yy\", u_yy.shape)\n",
    "        # print(\"torch.pow(torch.abs(u), 2)*u\", (torch.pow(torch.abs(u), 2)*u).shape)\n",
    "       \n",
    "        f = u_t - myu*u - (u_xx + u_yy) + torch.pow(torch.abs(u), 2)*u\n",
    "        \n",
    "        # free_memory(u_t, u_x, u_y, u_xx, u_yy)\n",
    "        return f\n",
    "    \n",
    "    def loss_bc(self, x, y, t):\n",
    "        \n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        u = self.net_u(x, y, t).view(200, 4, 4)\n",
    "        \n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_y = torch.autograd.grad(\n",
    "            u, y, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        # Under maintenance!\n",
    "        # boundary_error_x_lb = ((u_x_lb - u_x_lb_correct)**2).mean()\n",
    "        # boundary_error_x_ub = ((u_x_ub - u_x_ub_correct)**2).mean()\n",
    "        \n",
    "        # boundary_error_y_lb = ((u_y_lb - u_y_lb_correct)**2).mean()\n",
    "        # boundary_error_y_ub = ((u_y_ub - u_y_ub_correct)**2).mean()\n",
    "        \n",
    "        # boundary_error = boundary_error_x_lb + boundary_error_x_ub + \\\n",
    "        #                  boundary_error_y_lb + boundary_error_y_ub\n",
    "        \n",
    "        # Add boundary error term to your f function\n",
    "        \n",
    "        u_x_lb = u_x[self.lb]\n",
    "        u_x_ub = u_x[self.ub]\n",
    "\n",
    "        u_y_lb = u_y[self.lb]\n",
    "        u_y_ub = u_y[self.ub]\n",
    "        \n",
    "        # print(u_x_lb.shape)\n",
    "        \n",
    "        # u_x_lb_correct = torch.zeros_like(self.lb)\n",
    "        # u_x_ub_correct = torch.zeros_like(self.ub)\n",
    "        # u_y_lb_correct = torch.zeros_like(self.lb)\n",
    "        # u_y_ub_correct = torch.zeros_like(self.ub)\n",
    "\n",
    "        u_x_lb_correct = self.lb[0] # if the first coordinate of lb is x\n",
    "        u_x_ub_correct = self.ub[0] # if the first coordinate of ub is x\n",
    "        u_y_lb_correct = self.lb[1] # if the second coordinate of lb is y\n",
    "        u_y_ub_correct = self.ub[1] # if the second coordinate of ub is y\n",
    "\n",
    "        # print(u_x_lb_correct.shape)\n",
    "\n",
    "        boundary_error_x_lb = ((u_x_lb - u_x_lb_correct)**2).mean()\n",
    "        boundary_error_x_ub = ((u_x_ub - u_x_ub_correct)**2).mean()\n",
    "        \n",
    "        boundary_error_y_lb = ((u_y_lb - u_y_lb_correct)**2).mean()\n",
    "        boundary_error_y_ub = ((u_y_ub - u_y_ub_correct)**2).mean()\n",
    "        \n",
    "        boundary_error = boundary_error_x_lb + boundary_error_x_ub + \\\n",
    "                        boundary_error_y_lb + boundary_error_y_ub\n",
    "                        \n",
    "        return boundary_error\n",
    "    \n",
    "    def loss_func(self):        \n",
    "        \n",
    "        u_pred = self.net_u(self.x, self.y, self.t).view(200, 4, 4)\n",
    "        f_pred = self.net_f(self.x, self.y, self.t)\n",
    "        loss_bc = self.loss_bc(self.x, self.y, self.t)\n",
    "        \n",
    "        u_loss = torch.mean(torch.abs((self.u - u_pred) ** 2))\n",
    "        f_loss = torch.mean(torch.abs(f_pred) ** 2)\n",
    "        bc_loss = torch.mean(torch.abs(loss_bc) ** 2)\n",
    "        \n",
    "        if self.lambda_reg is not None:\n",
    "            myu_reg = self.lambda_reg * torch.sum(self.myu)\n",
    "            loss = u_loss + f_loss + myu_reg\n",
    "        else:\n",
    "            loss = u_loss + f_loss + bc_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        if self.clipping:\n",
    "            self.update_myu()\n",
    "        \n",
    "        self.optimizer_counter += 1\n",
    "        # if self.optimizer_counter % 10 == 0:\n",
    "        #     print('Iter: %d, u_loss: %s, f_loss: %s, total loss: %s, myu:\\n %s \\n' % \n",
    "        #             (\n",
    "        #                 self.optimizer_counter,\n",
    "        #                 u_loss.item(),\n",
    "        #                 f_loss.item(),\n",
    "        #                 loss.item(), \n",
    "        #                 self.myu.cpu().detach().numpy()\n",
    "        #             )\n",
    "        #         )\n",
    "            \n",
    "        free_memory(u_loss, f_loss, u_pred, f_pred)\n",
    "        return loss\n",
    "\n",
    "    def train(self, epochs):\n",
    "        self.dnn.train()\n",
    "        \n",
    "        self.history = {\n",
    "            'u_loss': [],\n",
    "            'f_loss': [],\n",
    "            'total_loss': [],\n",
    "            'myu': []\n",
    "        }\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            \n",
    "            u_pred = self.net_u(self.x, self.y, self.t).view(200, 4, 4)\n",
    "            f_pred = self.net_f(self.x, self.y, self.t)\n",
    "            loss_bc = self.loss_bc(self.x, self.y, self.t)\n",
    "            \n",
    "            u_loss = torch.mean(torch.abs((self.u - u_pred) ** 2))\n",
    "            f_loss = torch.mean(torch.abs(f_pred) ** 2)\n",
    "            bc_loss = torch.mean(torch.abs(loss_bc) ** 2)\n",
    "            \n",
    "            # store the initial losses\n",
    "            if epoch == 0:\n",
    "                self.initial_f_loss = f_loss.item()\n",
    "                self.initial_u_loss = u_loss.item()\n",
    "                \n",
    "            # check early stopping conditions\n",
    "            if (self.initial_f_loss / f_loss.item() >= 1000 and \n",
    "                self.initial_u_loss - u_loss.item() >= 10):\n",
    "                print('It: %d, u_loss: %s, f_loss: %s, total loss: %s unique myus: %s' % \n",
    "                       (\n",
    "                        epoch,\n",
    "                        u_loss.item(),\n",
    "                        f_loss.item(),\n",
    "                        loss.item(), \n",
    "                        self.dnn.myu.cpu().detach()\n",
    "                    )\n",
    "                )\n",
    "                print(\"Early stopping condition reached.\")\n",
    "                break\n",
    "            \n",
    "            if self.lambda_reg is not None:\n",
    "                myu_reg = self.lambda_reg * torch.sum(self.myu)\n",
    "                loss = u_loss + f_loss + myu_reg\n",
    "            else:\n",
    "                loss = u_loss + f_loss + bc_loss\n",
    "\n",
    "            self.history['u_loss'].append(u_loss.item())\n",
    "            self.history['f_loss'].append(f_loss.item())\n",
    "            self.history['total_loss'].append(loss.item())\n",
    "            self.history['myu'].append(self.dnn.myu.cpu().detach().numpy())\n",
    "            \n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optimizer_Adam.step()\n",
    "            \n",
    "            if self.clipping:\n",
    "                self.update_myu()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print('It: %d, u_loss: %s, f_loss: %s, total loss: %s unique myus: %s' % \n",
    "                       (\n",
    "                        epoch,\n",
    "                        u_loss.item(),\n",
    "                        f_loss.item(),\n",
    "                        loss.item(), \n",
    "                        np.unique(self.dnn.myu.cpu().detach().numpy())\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            free_memory(u_loss, f_loss, u_pred)\n",
    "            if epoch % 30 == 0:\n",
    "                self.optimizer.step(self.loss_func)\n",
    "                        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True, dtype=torch.complex64).reshape(200, 4, 4).to(device)\n",
    "        y = torch.tensor(X[:, 1:2], requires_grad=True, dtype=torch.complex64).reshape(200, 4, 4).to(device)\n",
    "        t = torch.tensor(X[:, 2:3], requires_grad=True, dtype=torch.complex64).reshape(200, 4, 4).to(device)  \n",
    "        \n",
    "        self.dnn.eval()\n",
    "        \n",
    "        u = self.net_u(x, y, t)\n",
    "        f = self.net_f(x, y, t)\n",
    "        \n",
    "        u = u.view(parameters.mem_rat, parameters.Nx, parameters.Ny)\n",
    "        \n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        \n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "681f9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "x = np.linspace(0, Lx, Nx).flatten()[:, None]\n",
    "y = np.linspace(0, Ly, Ny).flatten()[:, None]\n",
    "t = np.linspace(0, T_end, N_ITERATIONS).flatten()[:, None]\n",
    "\n",
    "Exact = A_original.squeeze(0)\n",
    "\n",
    "X, T, Y = np.meshgrid(x, t, y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact\n",
    "\n",
    "# boundary conditions\n",
    "lb = X_star.min(0) # lower bound\n",
    "ub = X_star.max(0) # upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37882cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "model = PINN(\n",
    "    X=X_star, \n",
    "    u=u_star.reshape(-1, 1),\n",
    "    layers=snn_layers, \n",
    "    lb=lb, \n",
    "    ub=ub,\n",
    "    lambda_reg=None,\n",
    "    optimizer=torch.optim.LBFGS,\n",
    "    myu_bound=None, #np.max(parameters.myu_in[:, :, :]),\n",
    "    clipping=False\n",
    ")\n",
    "\n",
    "# gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2e0f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, u_loss: 62.916561126708984, f_loss: 72806361154.21222, total loss: 72809939512.12878 unique myus: [-2.18773146 -1.63068646 -1.56481671 -1.54941492 -1.32679304 -0.91235892\n",
      " -0.24492827  0.24619221]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3500 [00:23<35:33,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 10, u_loss: 380.27392578125, f_loss: 6.652619444563587e+27, total loss: 6.652625439548245e+27 unique myus: [-4.19684460e+04 -1.03091118e+04 -5.78845165e+03  2.01840528e+03\n",
      "  1.12527357e+04  3.28476963e+04  3.10908303e+05  8.27390397e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/3500 [00:27<24:21,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 20, u_loss: 834.6639404296875, f_loss: 6.652660965226033e+27, total loss: 6.652666895130299e+27 unique myus: [-4.19692910e+04 -1.03059356e+04 -5.79311871e+03  2.01091735e+03\n",
      "  1.12616955e+04  3.28607650e+04  3.10920066e+05  8.27389907e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/3500 [00:31<28:29,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 30, u_loss: 1927.641845703125, f_loss: 6.652660696518822e+27, total loss: 6.652666551803509e+27 unique myus: [-4.19582879e+04 -1.02926230e+04 -5.78305849e+03  2.01723597e+03\n",
      "  1.12761919e+04  3.28755077e+04  3.10934802e+05  8.27390909e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 41/3500 [01:05<37:36,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 40, u_loss: 63.41550064086914, f_loss: 1.452020772371951e+23, total loss: 1.4520210846001176e+23 unique myus: [1.99458832e+08 3.17315740e+08 3.71375959e+08 6.02058752e+08\n",
      " 8.87182907e+08 1.45078355e+09 2.03855448e+09 1.08924915e+11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 51/3500 [01:09<25:57,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 50, u_loss: 63.4163703918457, f_loss: 1.4520208184995753e+23, total loss: 1.452021130727742e+23 unique myus: [1.99458836e+08 3.17315744e+08 3.71375963e+08 6.02058755e+08\n",
      " 8.87182910e+08 1.45078355e+09 2.03855448e+09 1.08924915e+11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/3500 [01:15<27:51,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 60, u_loss: 63.41648483276367, f_loss: 1.4520208217120325e+23, total loss: 1.4520211339401991e+23 unique myus: [1.99458838e+08 3.17315745e+08 3.71375964e+08 6.02058756e+08\n",
      " 8.87182911e+08 1.45078356e+09 2.03855449e+09 1.08924915e+11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/3500 [01:32<1:28:29,  1.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m number_of_iteration \u001b[39m=\u001b[39m \u001b[39m3500\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mtrain(number_of_iteration)\n",
      "Cell \u001b[1;32mIn[10], line 300\u001b[0m, in \u001b[0;36mPINN.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    298\u001b[0m free_memory(u_loss, f_loss, u_pred)\n\u001b[0;32m    299\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m30\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func)\n",
      "File \u001b[1;32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python37-32\\envs\\conda_env\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python37-32\\envs\\conda_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mComplexOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m# Optimize real and imaginary parts separately\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m---> 44\u001b[0m     real_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreal_optimizer\u001b[39m.\u001b[39;49mstep(real_closure)\n\u001b[0;32m     45\u001b[0m     imag_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimag_optimizer\u001b[39m.\u001b[39mstep(imag_closure)\n\u001b[0;32m     47\u001b[0m \u001b[39m# Update the original complex parameters with the optimized real and imaginary parts\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python37-32\\envs\\conda_env\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python37-32\\envs\\conda_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python37-32\\envs\\conda_env\\lib\\site-packages\\torch\\optim\\lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mif\u001b[39;00m n_iter \u001b[39m!=\u001b[39m max_iter:\n\u001b[0;32m    434\u001b[0m     \u001b[39m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[0;32m    435\u001b[0m     \u001b[39m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 438\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(closure())\n\u001b[0;32m    439\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    440\u001b[0m     opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
      "File \u001b[1;32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python37-32\\envs\\conda_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mComplexOptimizer.step.<locals>.real_closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreal_closure\u001b[39m():\n\u001b[1;32m---> 30\u001b[0m     real_loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m real_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m         real_loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[10], line 196\u001b[0m, in \u001b[0;36mPINN.loss_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m u_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_u(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt)\u001b[39m.\u001b[39mview(\u001b[39m200\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m    195\u001b[0m f_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_f(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt)\n\u001b[1;32m--> 196\u001b[0m loss_bc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_bc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt)\n\u001b[0;32m    198\u001b[0m u_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mabs((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu \u001b[39m-\u001b[39m u_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m))\n\u001b[0;32m    199\u001b[0m f_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mabs(f_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 136\u001b[0m, in \u001b[0;36mPINN.loss_bc\u001b[1;34m(self, x, y, t)\u001b[0m\n\u001b[0;32m    133\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    134\u001b[0m u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_u(x, y, t)\u001b[39m.\u001b[39mview(\u001b[39m200\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m--> 136\u001b[0m u_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(\n\u001b[0;32m    137\u001b[0m     u, x, \n\u001b[0;32m    138\u001b[0m     grad_outputs\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(u),\n\u001b[0;32m    139\u001b[0m     retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    140\u001b[0m     create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    141\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    142\u001b[0m u_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(\n\u001b[0;32m    143\u001b[0m     u, y, \n\u001b[0;32m    144\u001b[0m     grad_outputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones_like(u),\n\u001b[0;32m    145\u001b[0m     retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m     create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    149\u001b[0m \u001b[39m# Under maintenance!\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39m# boundary_error_x_lb = ((u_x_lb - u_x_lb_correct)**2).mean()\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39m# boundary_error_x_ub = ((u_x_ub - u_x_ub_correct)**2).mean()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m# Add boundary error term to your f function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python37-32\\envs\\conda_env\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_of_iteration = 3500\n",
    "model.train(number_of_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(model.history['u_loss']))\n",
    "print(np.min(model.history['f_loss']))\n",
    "print(np.max(model.history['myu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a41a52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# skip_size = 1000\n",
    "# fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "# fig.subplots_adjust(wspace=0.4, hspace=0.4, top=0.9)\n",
    "\n",
    "# axs[0,0].plot(iterations, model.history['f_loss'], label=\"f loss\")\n",
    "# axs[0,0].set_xlabel('Number of iterations')\n",
    "# axs[0,0].set_ylabel('Loss')\n",
    "# axs[0,0].set_title(\"f loss\")\n",
    "\n",
    "# axs[0,1].plot(iterations, model.history['u_loss'], label=\"u loss\")\n",
    "# axs[0,1].set_xlabel('Number of iterations')\n",
    "# axs[0,1].set_ylabel('Loss')\n",
    "# axs[0,1].set_title(\"u loss\")\n",
    "\n",
    "# # axs[0,2].plot(iterations, model.history['total_loss'], label=\"total loss\")\n",
    "# # axs[0,2].set_xlabel('Number of iterations')\n",
    "# # axs[0,2].set_ylabel('total loss')\n",
    "# # axs[0,2].set_title(\"total loss\")\n",
    "\n",
    "# axs[0,2].plot(iterations, model.history['myu'], label=\"myu\")\n",
    "# axs[0,2].axhline(y=np.unique(parameters.myu_in), color='r', linestyle='--', label=f'myu target')\n",
    "# axs[0,2].set_xlabel('Number of iterations')\n",
    "# axs[0,2].set_ylabel('myu')\n",
    "# axs[0,2].set_title(\"myu\")\n",
    "\n",
    "# axs[1,0].plot(iterations[skip_size:], model.history['f_loss'][skip_size:], label=\"f loss\")\n",
    "# axs[1,0].set_xlabel('Number of iterations')\n",
    "# axs[1,0].set_ylabel('Loss')\n",
    "# axs[1,0].set_title(\"f loss_\")\n",
    "\n",
    "# axs[1,1].plot(iterations[skip_size:], model.history['u_loss'][skip_size:], label=\"u loss\")\n",
    "# axs[1,1].set_xlabel('Number of iterations')\n",
    "# axs[1,1].set_ylabel('Loss')\n",
    "# axs[1,1].set_title(\"u loss_\")\n",
    "\n",
    "# # axs[1,2].plot(iterations[skip_size:], model.history['total_loss'][skip_size:], label=\"total loss\")\n",
    "# # axs[1,2].set_xlabel('Number of iterations')\n",
    "# # axs[1,2].set_ylabel('total loss')\n",
    "# # axs[1,2].set_title(\"total loss_\")\n",
    "\n",
    "# axs[1,2].plot(iterations[skip_size:], model.history['myu'][skip_size:], label=\"myu pred\")\n",
    "# axs[1,2].axhline(y=np.unique(parameters.myu_in), color='r', linestyle='--', label=f'myu target')\n",
    "# axs[1,2].set_xlabel('Number of iterations')\n",
    "# axs[1,2].set_ylabel('myu')\n",
    "# axs[1,2].set_title(\"myu_\")\n",
    "\n",
    "# fig.suptitle(\"Model performance\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "iterations = np.linspace(0, len(model.history['f_loss']), len(model.history['f_loss']))\n",
    "skip_size = 1000\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 4))  # Only one row now\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.4, top=0.9)\n",
    "\n",
    "axs[0].plot(iterations, model.history['f_loss'], label=\"f loss\")\n",
    "axs[0].set_xlabel('Number of iterations')\n",
    "axs[0].set_ylabel('Log-scaled Loss')\n",
    "axs[0].set_title(\"f loss\")\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "axs[1].plot(iterations, model.history['u_loss'], label=\"u loss\")\n",
    "axs[1].set_xlabel('Number of iterations')\n",
    "axs[1].set_ylabel('Log-scaled Loss')\n",
    "axs[1].set_title(\"u loss\")\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# axs[2].plot(iterations, model.history['myu'], label=\"myu\")\n",
    "# axs[2].axhline(y=np.unique(parameters.myu_in), color='r', linestyle='--', label=f'myu target')\n",
    "# axs[2].set_xlabel('Number of iterations')\n",
    "# axs[2].set_ylabel('Log-scaled myu')\n",
    "# axs[2].set_title(\"myu\")\n",
    "# axs[2].set_yscale('log')\n",
    "\n",
    "fig.suptitle(\"Model performance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3977115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myu_history = [np.array(myu).flatten() for myu in model.history['myu']] # Flatten each myu to a 1D array\n",
    "# myu_components = list(zip(*myu_history))\n",
    "# myu_in_values_flattened = np.array(np.unique(parameters.myu_in)).flatten()\n",
    "\n",
    "# color_cycle = cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n",
    "\n",
    "# myu_tensor_shape = model.dnn.myu.shape\n",
    "# num_rows, num_cols, _ = myu_tensor_shape\n",
    "\n",
    "# fig, axs = plt.subplots(len(np.unique(myu_history)), len(np.unique(myu_history)), figsize=(15, 8))\n",
    "# fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# for i in tqdm(range(len(np.unique(myu_history)))):\n",
    "#     for j in tqdm(range(len(np.unique(myu_history)))):\n",
    "#         # Calculate the flat index for the myu component and target value\n",
    "#         flat_idx = i * num_cols + j\n",
    "\n",
    "#         axs[i, j].plot(iterations, myu_components[flat_idx])\n",
    "#         color = next(color_cycle)\n",
    "#         axs[i, j].axhline(y=myu_in_values_flattened[flat_idx], color=color, linestyle='--', label=f'myu target {i+1}, {j+1}')\n",
    "#         axs[i, j].set_title(f'Myu {i+1}, {j+1}')\n",
    "#         axs[i, j].set_xlabel('Number of iterations')\n",
    "#         axs[i, j].set_ylabel('myu')\n",
    "#         axs[i, j].legend()\n",
    "\n",
    "# fig.suptitle(\"Model performance\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda7817",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Calculate max of loss histories\n",
    "# max_u_loss = np.max(model.history['u_loss'])\n",
    "# max_f_loss = np.max(model.history['f_loss'])\n",
    "# max_total_loss = np.max(model.history['total_loss'])\n",
    "\n",
    "# # Normalize loss histories\n",
    "# f_loss_norm = (model.history['f_loss'] - np.min(model.history['f_loss'])) / (max_f_loss - np.min(model.history['f_loss']))\n",
    "# u_loss_norm = (model.history['u_loss'] - np.min(model.history['u_loss'])) / (max_u_loss - np.min(model.history['u_loss']))\n",
    "# total_loss_norm = (model.history['total_loss'] - np.min(model.history['total_loss'])) / (max_total_loss - np.min(model.history['total_loss']))\n",
    "\n",
    "# skip_size = 3000\n",
    "# fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "# fig.subplots_adjust(wspace=0.4, hspace=0.4, top=0.9)\n",
    "\n",
    "# # Plotting normalized total loss, u_loss and f_loss with skip size\n",
    "# axs[0,0].plot(iterations[skip_size:], total_loss_norm[skip_size:], label=\"total loss norm\")\n",
    "# axs[0,1].plot(iterations[skip_size:], u_loss_norm[skip_size:], label=\"u loss norm\")\n",
    "# axs[0,2].plot(iterations[skip_size:], f_loss_norm[skip_size:], label=\"f loss norm\")\n",
    "\n",
    "# # Plotting normalized total loss, u_loss and f_loss for all iterations\n",
    "# axs[1,0].plot(iterations, total_loss_norm, label=\"total loss norm\")\n",
    "# axs[1,1].plot(iterations, u_loss_norm, label=\"u loss norm\")\n",
    "# axs[1,2].plot(iterations, f_loss_norm, label=\"f loss norm\")\n",
    "\n",
    "# # Labeling axes and setting titles\n",
    "# for i in range(2):\n",
    "#     for j in range(3):\n",
    "#         axs[i,j].set_xlabel('Number of iterations')\n",
    "#         axs[i,j].set_ylabel('Normalized Loss')\n",
    "#         axs[i,j].legend()\n",
    "\n",
    "# axs[0,0].set_title(\"total loss norm (skipped)\")\n",
    "# axs[0,1].set_title(\"u loss norm (skipped)\")\n",
    "# axs[0,2].set_title(\"f loss norm (skipped)\")\n",
    "# axs[1,0].set_title(\"total loss norm\")\n",
    "# axs[1,1].set_title(\"u loss norm\")\n",
    "# axs[1,2].set_title(\"f loss norm\")\n",
    "\n",
    "# fig.suptitle(\"Model performance (normalized values)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577beaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # evaluations\n",
    "# u_pred, f_pred = model.predict(X_star)\n",
    "# u_pred = torch.tensor(u_pred)\n",
    "# u_test = torch.tensor(u_star)\n",
    "# # error_u = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
    "\n",
    "# myu_model = np.array(model.dnn.myu.detach().cpu()).flatten()\n",
    "# myu_target = np.array(np.unique(parameters.myu_in)).flatten()\n",
    "\n",
    "# # # Ensure that myu_model and myu_target are of the same size\n",
    "# # assert len(myu_model) == len(myu_target), \"Sizes of myu_model and myu_target don't match\"\n",
    "\n",
    "# myu_error = (myu_model / myu_target) * 100\n",
    "\n",
    "# print(\"Myu model: \", myu_model.tolist())\n",
    "# print(\"Myu target: \", myu_target.tolist())\n",
    "# print('Myu percentage of the target: ', myu_error.tolist())\n",
    "\n",
    "# # myu_target = np.unique(parameters.myu_in)\n",
    "# # myu_error = (model.myu.detach().cpu().numpy() * 100) / myu_target\n",
    "# # print('Myu percentage of the target: ', myu_error) \n",
    "# # print('Error u: ', error_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(np.unique(model.dnn.myu.detach().cpu().numpy())))\n",
    "print(np.count_nonzero(np.unique(parameters.myu_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac11c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u_pred, f_pred = model.predict(X_star)\n",
    "print(u_pred.shape, f_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf2c78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path=\"combo_test\"\n",
    "create_gifs(\n",
    "    memory_rate=parameters.mem_rat, \n",
    "    u_pred=u_pred, \n",
    "    original=parameters.A_original, \n",
    "    save=True, \n",
    "    path_for_gif=path+\".gif\", \n",
    "    duration=500, \n",
    "    title=\" \"\n",
    ")\n",
    "Image(filename=path+\".gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif = imageio.mimread(path+\".gif\", memtest=False)\n",
    "nums = len(gif)\n",
    "print(\"Total {} frames in the gif {}!\".format(nums, path+\".gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53186a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert form BGR to RGB\n",
    "imgs = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in gif]\n",
    "\n",
    "# Save frames to video\n",
    "out = cv2.VideoWriter(path+'.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 2, (imgs[0].shape[1], imgs[0].shape[0]))\n",
    "\n",
    "for img in imgs:\n",
    "    out.write(img)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d637bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
